data:
    dataset_type: "cached_threedfront"
    encoding_type: "cached_diffusion_text_cosin_angle_objfeatsnorm_lat32_wocm" # textfix for eval  "cached_diffusion_textfix_cosin_angle_objfeatsnorm_lat32_wocm"
    dataset_directory: "/cluster/balrog/jtang/3d_front_processed/bedrooms_objfeats_32_64"
    annotation_file: "../config/bedroom_threed_front_splits.csv"
    path_to_invalid_scene_ids: "../config/invalid_threed_front_rooms.txt"
    path_to_invalid_bbox_jids: "../config/black_list.txt"
    augmentations: ["fixed_rotations"]
    filter_fn: "threed_front_bedroom"
    train_stats: "dataset_stats.txt"
    room_layout_size: "64,64"


network:
    type: "diffusion_scene_layout_ddpm"
    # denoising network
    net_type: "unet1d"

    # concate squarewish layer
    point_dim: 62
    latent_dim: 0
    room_mask_condition: false # not use room_mask 
    text_condition: True
    text_embed_dim: 512
    sample_num_points: 12 # max_length 

    objectness_dim: 0
    class_dim: 22
    angle_dim: 2
    objfeat_dim: 32 #64

    # class condition
    learnable_embedding: true
    instance_condition: true
    instance_emb_dim: 128

    # diffusion config
    diffusion_kwargs:
        schedule_type: 'linear'
        beta_start: 0.0001
        beta_end: 0.02
        time_num: 1000 
        loss_type: 'mse'
        model_mean_type: 'v' #'x0' #'eps'
        model_var_type: 'fixedsmall'
        loss_separate: true
        # calculate iou loss
        loss_iou: true
        train_stats_file: "/cluster/balrog/jtang/3d_front_processed/bedrooms_objfeats/dataset_stats.txt"

    net_kwargs:
        dim: 512
        dim_mults: [1, 1, 1, 1]
        channels: 62
        objectness_dim: 0
        class_dim: 22
        angle_dim: 2
        objfeat_dim: 32
        self_condition: true
        context_dim: 0
        instanclass_dim: 128 
        seperate_all: true  # separate all
        merge_bbox: true 
        modulate_time_context_instanclass: true
        text_condition: True
        text_dim: 512

feature_extractor:
    name: "resnet18"
    feature_size: 64
    freeze_bn: true
    input_channels: 1

training:
    splits: ["train", "val"]
    epochs: 60000
    steps_per_epoch: 500
    batch_size: 128
    save_frequency: 2000
    max_grad_norm: 10
    # optimizer
    optimizer: Adam
    weight_decay: 0.0
    # schedule
    schedule: 'step'
    lr: 0.0002
    lr_step: 10000
    lr_decay: 0.5
    # schedule: 'lambda'
    # start_lr: 0.0002
    # end_lr: 0.000001
    # start_epoch: 1000 # 200k iter / 400 epochs -- 500 iter /epoch
    # end_epoch: 4000 # 400k iter / 800 epochs

validation:
    splits: ["test"]
    frequency: 10
    batch_size: 128
    gen_traj: false
    num_step: 100
    gen_gt:  true #false
    gen_prob_map: false

logger:
    type: "wandb"
    project: "diffuscene"
 